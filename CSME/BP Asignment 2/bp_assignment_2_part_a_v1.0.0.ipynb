{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# DSME Bonus Point Assignment II Part A\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.0, released 2021-01-29</div>\n",
    "For task instructions, refer to the assignment PDF.\n",
    "\n",
    "* The parts of the code you are to implement are indicated via `# TODO` comments.\n",
    "* You can use the `# Test code` cells to verify your implementation. However note that these are not the unit tests used for grading.\n",
    "* Some cells create export file in the `solution/` folder. _Include whole `solution/` folder in your submission_.\n",
    "* DO NOT CLEAR THE OUTPUT of the notebook you are submitting!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# Create solution folder\n",
    "Path(\"solution/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "source": [
    "## Question A1 - Q-Tables\n",
    "### a) The FrozenLake environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# ********************\n",
    "# TODO Create environment\n",
    "# env = \n",
    "\n",
    "\n",
    "# TODO Render initial state\n",
    "\n",
    "\n",
    "# ********************"
   ]
  },
  {
   "source": [
    "### b) Manual Navigation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.toy_text.frozen_lake import LEFT, DOWN, RIGHT, UP  # Import action constants\n",
    "\n",
    "# Reset frozenlake env to reproducible state\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "env.render('human')\n",
    "\n",
    "# ********************\n",
    "# TODO Find action sequence that leads to the goal\n",
    "\n",
    "\n",
    "# ********************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Code\n",
    "np.testing.assert_equal(env.s, 15, err_msg=\"Env not in goal state\")\n",
    "\"ok\""
   ]
  },
  {
   "source": [
    "### c) Building the Q-Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************\n",
    "# TODO Initialize Q-table of size 16x4\n",
    "# Q = ...\n",
    "\n",
    "\n",
    "# ********************\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.8\n",
    "gamma = 0.95\n",
    "num_episodes = 2000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "# Create lists to contain total rewards and steps per episode\n",
    "rewards = np.zeros(num_episodes)\n",
    "\n",
    "for i in tqdm(range(num_episodes), desc=\"Training\"):\n",
    "    # Reset environment and observe initial state\n",
    "    s = env.reset()\n",
    "\n",
    "    # The Q-Table learning algorithm\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        # ********************\n",
    "        # TODO Choose action\n",
    "        # a = ...\n",
    "\n",
    "\n",
    "        # ********************\n",
    "\n",
    "        # Get new state and reward from environment\n",
    "        s_new, r, done, _ = env.step(a)\n",
    "\n",
    "        # ********************\n",
    "        # TODO Update Q-table\n",
    "        # Q[s, a] = ...\n",
    "\n",
    "\n",
    "        # ********************\n",
    "\n",
    "        # Bookkeeping\n",
    "        rewards[i] += r\n",
    "        s = s_new\n",
    "\n",
    "        if done:  # Check if episode terminated\n",
    "            break\n",
    "\n",
    "# Plot rewards\n",
    "sns.lineplot(data=pd.DataFrame(rewards, columns=[\"training\"]).rolling(50).mean())\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/a1c-train.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Q-table\n",
    "plt.figure(figsize=(14, 3))\n",
    "sns.heatmap(Q.T, square=True, cbar_kws={'label': 'Q value'},yticklabels=[\"LEFT\", \"DOWN\", \"RIGHT\", \"UP\"])\n",
    "plt.xlabel(\"state\")\n",
    "plt.ylabel(\"action\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/a1c-q-values.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Code\n",
    "\n",
    "def rollout(env, Q, seed=None):\n",
    "    \"\"\"Perform single rollout\"\"\"\n",
    "    reward = 0\n",
    "    env.seed(seed)\n",
    "    s = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        # Choose action greedily from Q-table\n",
    "        a = np.argmax(Q[s, :])\n",
    "        s, r, done, _ = env.step(a)\n",
    "        reward += r\n",
    "        if done:  # Check if episode terminated\n",
    "            break\n",
    "    return reward\n",
    "\n",
    "validation_reward = np.mean([rollout(env, Q, i) for i in range(100)])\n",
    "print(f\"Validation reward: {validation_reward}\", )\n",
    "assert validation_reward > 0.5, f\"Average reward of learned Q-table should be greater than 0.5\"\n",
    "\"ok\""
   ]
  },
  {
   "source": [
    "## Question A2 - Policy Gradients"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gym \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    " "
   ]
  },
  {
   "source": [
    "Note: The next cell is optional, as this will _not_ run on the JupterHub. To render the CartPole environemnt, you need to set up your Jupyter environment locally (see assignment PDF). Rendering is not required for this assignment, but the visualization may help you to understand what your policy is atually learning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "for _ in range(10):\n",
    "    state, done = env.reset(), False\n",
    "    while not done: \n",
    "        action = env.action_space.sample() # Random action\n",
    "        state, _, done, _ = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "source": [
    "### a) Defining the Policy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # TODO Create layers\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO Implement forward pass\n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code\n",
    "_test_output = Policy()(torch.tensor([[1.0,2,3,4]]))\n",
    "assert _test_output.shape == (1, 2), f\"Expected output shape (1, 2), got {_test_output.shape}\"\n",
    "np.testing.assert_almost_equal(_test_output.detach().numpy().sum(), 1, err_msg=\"Output is not a probability distribution.\")\n",
    "\"ok\""
   ]
  },
  {
   "source": [
    "### b) Action Sampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(probs):\n",
    "    \"\"\"Sample one action from the action distribution of this state.\n",
    "    \n",
    "    Args:\n",
    "        probs: action probabilities\n",
    "\n",
    "    Returns:\n",
    "        action: The sampled action\n",
    "        log_prob: Logarithm of the probability for sampling that action\n",
    "    \"\"\"\n",
    "    # TODO Implement action sampling\n",
    "\n",
    "    return action, log_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code\n",
    "_test_action, _test_log_prob = sample_action(torch.tensor([1,2,3,4]))\n",
    "assert _test_action in [0, 1, 2, 3], f\"Invalid action {_test_action}\"\n",
    "np.testing.assert_approx_equal(_test_log_prob, np.log((_test_action+1)/10))\n",
    "\"ok\""
   ]
  },
  {
   "source": [
    "### c)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_return(rewards, gamma=0.99):\n",
    "     \"\"\"Estimate return based of observed rewards\n",
    "    \n",
    "    Args:\n",
    "        rewards: Series of observed rewards\n",
    "        gamma: discount factor\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code\n",
    "np.testing.assert_array_almost_equal(\n",
    "     estimate_return(np.ones(10), gamma=0.99), \n",
    "    [1.54572815, 1.21139962, 0.87369404, 0.53257729, 0.18801491,-0.16002789, -0.51158628, -0.86669576, -1.22539221, -1.58771185])\n",
    "\"ok\""
   ]
  },
  {
   "source": [
    "### d) Training Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "policy = Policy()\n",
    "\n",
    "# Hyperparams\n",
    "episodes = 1000\n",
    "gamma = 0.99\n",
    "learn_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=learn_rate)\n",
    "\n",
    "total_rewards = []\n",
    "with tqdm(range(episodes)) as pbar:\n",
    "    for _ in pbar:\n",
    "        # Run one episode\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        state, done = env.reset(), False\n",
    "        while not done:\n",
    "            # Take a step\n",
    "            # ********************\n",
    "            # TODO Sample action for current state\n",
    "            \n",
    "            \n",
    "            # action, log_prob = ...\n",
    "\n",
    "\n",
    "            # ********************\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Bookkeeping\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "        total_rewards.append(sum(rewards))\n",
    "\n",
    "\n",
    "        # ********************\n",
    "        # TODO Compute loss\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # policy_loss = ...\n",
    "\n",
    "\n",
    "        # ********************\n",
    "\n",
    "        # Update policy\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "         # Print statistics\n",
    "        pbar.set_description(f\"Mean training reward {np.mean(total_rewards[-100:]):.02f}\")\n",
    "\n",
    "\n",
    "# Save model\n",
    "with open(\"solution/a2d.pt\", \"wb\") as f:\n",
    "    torch.save(policy, f)\n",
    "\n",
    "# Plot training\n",
    "plt.plot(total_rewards, label=\"per episode\")\n",
    "plt.plot(pd.DataFrame(total_rewards).rolling(100).mean(), label=\"average reward\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/a2d.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code\n",
    "policy.eval() # Switch to evaluation mode\n",
    "\n",
    "def _rollout(seed):\n",
    "    env.seed(seed)\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        probs = policy(torch.tensor(state).float().reshape((1, -1)))[0]\n",
    "        action = np.argmax(probs.detach().numpy())  # Greedy action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "    return env._elapsed_steps\n",
    "        \n",
    "_avg_reward = np.mean([_rollout(seed=i) for i in tqdm(range(100), desc=\"Validating\")])\n",
    "assert _avg_reward >= 487.5, f\"Average reward below 487.5, got {_avg_reward}\"\n",
    "f\"ok (Average reward {_avg_reward:0.2f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the learned policy (this will not run on the JupyterHub)\n",
    "greedy = True\n",
    "\n",
    "policy.eval() # Switch to evaluation mode\n",
    "state, done = env.reset(), False\n",
    "while not done: \n",
    "    probs = policy(torch.tensor(state).float().reshape((1, -1)))[0]\n",
    "    if greedy:\n",
    "        action = np.argmax(probs.detach().numpy())  # Chose optimal action\n",
    "    else:\n",
    "        action = sample_action(probs)[0]  # Sample from action distribution\n",
    "    state, _, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}